# Learning Sequential Data 

## Overview 
Human Lives are but un-solved sequences, but of-course we want AI to learn them ! 

## Outline
### Previous Talks 
- Spring 2019 : [Sequences and Language Models](https://docs.google.com/presentation/d/1P9yYs8fx3b-Ps0WDhrCSzQvL071u4Gx6k-kFr2-fg8c/edit?usp=sharing)
- Fall 2019 : [Graphs and Stuff](https://docs.google.com/presentation/d/1wIqxtu28Pbeg1NjwxnCl1fkOQOnJVY7PSXhosy5ZJPY/edit?usp=sharing)

#### Resources

##### Attention and Language Models
###### Papers
- [Transformers : Attention is all you need](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) 
- [The Annotated Transformer ](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Pretraining Language Models](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

###### Blogs n Tutorials
- [Understanding Attention Mechanism](https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f)
- [Lil's Log: Attention ](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) 
- [Pytorch Transformer Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [Attention Mechanism n Memory Networks : skymind.ai](https://skymind.ai/wiki/attention-mechanism-memory-network)

###### Vids
- [Attention Model : deeplearning.ai](https://www.youtube.com/watch?v=quoGRI-1l0A)
- [Attention and Memory in Deep Learning : deepMind google](https://www.youtube.com/watch?v=Q57rzaHHO0k)
- [Attention Paper Review](https://www.youtube.com/watch?v=iDulhoQ2pro)
- [Bert Explained](https://www.youtube.com/watch?v=0EtD5ybnh_s)

##### Graphs

###### Papers
- [Graphs Survey 1](https://arxiv.org/pdf/1812.04202.pdf) 
- [Graphs Survey 2](https://arxiv.org/pdf/1901.00596.pdf) 
- [Graphs Survey 3](https://arxiv.org/pdf/1705.02801.pdf) 
- [Representation Learning on Graphs: Methods and Applications](https://www-cs.stanford.edu/people/jure/pubs/graphrepresentation-ieee17.pdf)

###### Blogs n Tutorials
- [Graph Convolution Neural Network](https://tkipf.github.io/graph-convolutional-networks/) 
- Stanford Network Representations 
	- [What is network representation learning and why is it important?](http://snap.stanford.edu/proj/embeddings-www/files/nrltutorial-part0-intro.pdf)
	- [Learning low-dimensional embeddings of nodes in complex networks (e.g., DeepWalk and node2vec).](http://snap.stanford.edu/proj/embeddings-www/files/nrltutorial-part1-embeddings.pdf)
	- [Techniques for deep learning on network/graph structed data (e.g., graph convolutional networks and GraphSAGE).](http://snap.stanford.edu/proj/embeddings-www/files/nrltutorial-part2-gnns.pdf)
	- [Applications of network representation learning for recommender systems and computational biology.](http://snap.stanford.edu/proj/embeddings-www/files/nrltutorial-part3-applications.pdf)

 
###### Frameworks 
- [Deep Graph Library](https://www.dgl.ai/) 
- [Pytorch Geometric](https://github.com/rusty1s/pytorch_geometric) 

### Recurrent Neural Nets 
- [Original Recurrent Net](https://web.stanford.edu/class/psych209a/ReadingsByDate/02_25/Williams%20Zipser95RecNets.pdf)
- [RNN in Numpy and Passage Generation](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [Pytorch RNN Beginner 1](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)
- [Pytorch RNN Beginner 2](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)

### Long Short Term Memory 
- [Original Lstm Tutorial - Jurgen Schmidhuber](http://people.idsia.ch/~juergen/lstm/)
- [Lstm in Numpy from Scratch](https://blog.varunajayasiri.com/numpy_lstm.html)
- [Pytorch Lstm Beginner](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)
- [Pytorch Lstm Seq2Seq](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)

### Language Models
- [Computational Linguistics Cheat Sheet](https://github.com/crazysal/computational-linguistics/tree/master/cheatSheatsProlog) 

### Word Vectors 
- [Word2vec - Original Paper](https://arxiv.org/pdf/1301.3781.pdf)
- [Training Millions of Embedding Model](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) 
- [What are word embeddings](https://machinelearningmastery.com/what-are-word-embeddings/)
- [Understanding word2vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)
- [Word Embedding Applications](https://hackernoon.com/word-embeddings-in-nlp-and-its-applications-fab15eaf7430)
- [Word2vec using Gensim - Tutorial](https://www.guru99.com/word-embedding-word2vec.html)
- [How to use Pre-Trained Word2vec Embeddings in Pytorch](https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76)
- [ELMO - Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)
- [COVE; Learned in Translation: Contextualized Word Vectors](https://arxiv.org/pdf/1708.00107.pdf)

